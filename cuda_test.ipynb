{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Check if container is passing through + general imports \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Databricks Connection and Creating Dataset from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-----------+-----------+----------+----------+---------+-------------------+---------+----------------+----------------+----------+----------------+--------------+\n",
      "|ss_sold_date_sk|ss_item_sk|ss_store_sk|ss_promo_sk|ss_quantity|    d_date|d_day_name|d_holiday|d_following_holiday|d_weekend|       i_item_id|      s_store_id|p_promo_id|promo_start_date|promo_end_date|\n",
      "+---------------+----------+-----------+-----------+-----------+----------+----------+---------+-------------------+---------+----------------+----------------+----------+----------------+--------------+\n",
      "|        2451181|     14386|          1|        251|         77|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAACDIDAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     11323|          1|          1|         84|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAALDMCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     10141|          1|         44|         96|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAANJHCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      8059|          1|         72|         51|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAALHPBAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      7508|          1|         36|         96|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAEFNBAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     11696|          1|        242|         38|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAALNCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      2545|          1|        260|          7|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAABPJAAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      5708|          1|          2|         56|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAMEGBAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      7388|          1|        205|          7|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAMNMBAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      5464|          1|        216|         69|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAIFFBAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      8512|          1|        186|         47|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAAEBCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|       838|          1|         20|         82|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAGEDAAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     10832|          1|        191|         64|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAAFKCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      8972|          1|        244|         35|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAMADCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      8774|          1|        181|         73|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAGECCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     10496|          1|         49|         98|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAAAJCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      8743|          1|        156|         85|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAHCCCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     11215|          1|         40|        100|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAPMLCAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|      2083|          1|        210|         61|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAADCIAAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "|        2451181|     15322|          1|        168|         74|1999-01-02|  Saturday|        N|                  Y|        Y|AAAAAAAAKNLDAAAA|AAAAAAAABAAAAAAA|      NULL|            NULL|          NULL|\n",
      "+---------------+----------+-----------+-----------+-----------+----------+----------+---------+-------------------+---------+----------------+----------------+----------+----------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Make Connection\n",
    "from databricks_connect import connect_explicit\n",
    "spark = connect_explicit()\n",
    "\n",
    "#Making initial data query\n",
    "data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ss_sold_date_sk,\n",
    "        ss_item_sk,\n",
    "        ss_store_sk,\n",
    "        ss_promo_sk,\n",
    "        ss_quantity,\n",
    "        d_date,\n",
    "        d_day_name,\n",
    "        d_holiday,\n",
    "        d_following_holiday,\n",
    "        d_weekend,\n",
    "        i_item_id,\n",
    "        s_store_id\n",
    "        \n",
    "    FROM samples.tpcds_sf1.store_sales AS ss\n",
    "    INNER JOIN samples.tpcds_sf1.date_dim AS dd\n",
    "    ON ss.ss_sold_date_sk = dd.d_date_sk\n",
    "    INNER JOIN samples.tpcds_sf1.item AS i\n",
    "    ON ss.ss_item_sk = i.i_item_sk\n",
    "    INNER JOIN samples.tpcds_sf1.store AS s\n",
    "    ON ss.ss_store_sk = s.s_store_sk\n",
    "    \"\"\")\n",
    "\n",
    "# Promotion table with actual dates\n",
    "promo = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        p.p_item_sk, \n",
    "        p.p_promo_sk,\n",
    "        p.p_promo_id,\n",
    "        dd_start.d_date as promo_start_date,\n",
    "        dd_end.d_date as promo_end_date\n",
    "\n",
    "    FROM samples.tpcds_sf1.promotion AS p\n",
    "    LEFT JOIN samples.tpcds_sf1.date_dim AS dd_start\n",
    "    ON p.p_start_date_sk = dd_start.d_date_sk\n",
    "    LEFT JOIN samples.tpcds_sf1.date_dim AS dd_end\n",
    "    ON p.p_end_date_sk = dd_end.d_date_sk\n",
    "    \"\"\")\n",
    "\n",
    "sales_promo = data.join(\n",
    "    promo,\n",
    "    (data.ss_item_sk == promo.p_item_sk) & \n",
    "    (data.d_date >= promo.promo_start_date) & \n",
    "    (data.d_date <= promo.promo_end_date),\n",
    "    \"left\"\n",
    ").select(\n",
    "    data[\"*\"],\n",
    "    promo.p_promo_id,\n",
    "    promo.promo_start_date,\n",
    "    promo.promo_end_date\n",
    ")\n",
    "\n",
    "sales_promo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+-------+-----------------+-------+----------------+----------------+---------------+--------+-----------+---------+\n",
      "|      date|quantity|     day|holiday|following_holiday|weekend|         item_id|        store_id|promo_indicator|promo_id|promo_start|promo_end|\n",
      "+----------+--------+--------+-------+-----------------+-------+----------------+----------------+---------------+--------+-----------+---------+\n",
      "|1999-01-02|    77.0|Saturday|      N|                Y|      Y|AAAAAAAACDIDAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    84.0|Saturday|      N|                Y|      Y|AAAAAAAALDMCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    96.0|Saturday|      N|                Y|      Y|AAAAAAAANJHCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    51.0|Saturday|      N|                Y|      Y|AAAAAAAALHPBAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    96.0|Saturday|      N|                Y|      Y|AAAAAAAAEFNBAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    38.0|Saturday|      N|                Y|      Y|AAAAAAAAALNCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|     7.0|Saturday|      N|                Y|      Y|AAAAAAAABPJAAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    56.0|Saturday|      N|                Y|      Y|AAAAAAAAMEGBAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|     7.0|Saturday|      N|                Y|      Y|AAAAAAAAMNMBAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    69.0|Saturday|      N|                Y|      Y|AAAAAAAAIFFBAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    47.0|Saturday|      N|                Y|      Y|AAAAAAAAAEBCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    82.0|Saturday|      N|                Y|      Y|AAAAAAAAGEDAAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    64.0|Saturday|      N|                Y|      Y|AAAAAAAAAFKCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    35.0|Saturday|      N|                Y|      Y|AAAAAAAAMADCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    73.0|Saturday|      N|                Y|      Y|AAAAAAAAGECCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    98.0|Saturday|      N|                Y|      Y|AAAAAAAAAAJCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    85.0|Saturday|      N|                Y|      Y|AAAAAAAAHCCCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|   100.0|Saturday|      N|                Y|      Y|AAAAAAAAPMLCAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    61.0|Saturday|      N|                Y|      Y|AAAAAAAADCIAAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "|1999-01-02|    74.0|Saturday|      N|                Y|      Y|AAAAAAAAKNLDAAAA|AAAAAAAABAAAAAAA|              N|    NULL|       NULL|     NULL|\n",
      "+----------+--------+--------+-------+-----------------+-------+----------------+----------------+---------------+--------+-----------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Clean and aggregate data\n",
    "#spc = \"Sales Promo Cleaned\"\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum, when, col\n",
    "from pyspark.sql.types import DateType, StringType, FloatType\n",
    "\n",
    "spc = sales_promo.select(               sales_promo.d_date,\n",
    "                                        sales_promo.ss_quantity, \n",
    "                                        sales_promo.d_day_name,\n",
    "                                        sales_promo.d_holiday,\n",
    "                                        sales_promo.d_following_holiday,\n",
    "                                        sales_promo.d_weekend,\n",
    "                                        sales_promo.i_item_id,\n",
    "                                        sales_promo.s_store_id,\n",
    "                                        sales_promo.p_promo_id,\n",
    "                                        sales_promo.promo_start_date,\n",
    "                                        sales_promo.promo_end_date)\n",
    "\n",
    "spc = spc.withColumn(\n",
    "    \"promo_indicator\", \n",
    "        when(col(\"p_promo_id\").isNotNull(), \"Y\").otherwise(\"N\")\n",
    ")\n",
    "\n",
    "spc_typed = spc.select(\n",
    "    col(\"d_date\").cast(DateType()).alias(\"date\"),\n",
    "    col(\"ss_quantity\").cast(FloatType()).alias(\"quantity\"),\n",
    "    col(\"d_day_name\").cast(StringType()).alias(\"day\"),\n",
    "    col(\"d_holiday\").cast(StringType()).alias(\"holiday\"),\n",
    "    col(\"d_following_holiday\").cast(StringType()).alias(\"following_holiday\"),\n",
    "    col(\"d_weekend\").cast(StringType()).alias(\"weekend\"),\n",
    "    col(\"i_item_id\").cast(StringType()).alias(\"item_id\"),\n",
    "    col(\"s_store_id\").cast(StringType()).alias(\"store_id\"),\n",
    "    col(\"promo_indicator\").cast(StringType()).alias(\"promo_indicator\"),\n",
    "    col(\"p_promo_id\").cast(StringType()).alias(\"promo_id\"),\n",
    "    col(\"promo_start_date\").cast(DateType()).alias(\"promo_start\"),\n",
    "    col(\"promo_end_date\").cast(DateType()).alias(\"promo_end\")\n",
    ")\n",
    "\n",
    "spc_typed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-------+-----------------+-------+----------------+----------------+---------------+\n",
      "|      date|quantity|   day|holiday|following_holiday|weekend|         item_id|        store_id|promo_indicator|\n",
      "+----------+--------+------+-------+-----------------+-------+----------------+----------------+---------------+\n",
      "|1998-01-02|    84.0|Friday|      N|                Y|      Y|AAAAAAAAEAKDAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    86.0|Friday|      N|                Y|      Y|AAAAAAAAMDACAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    48.0|Friday|      N|                Y|      Y|AAAAAAAADNJAAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    36.0|Friday|      N|                Y|      Y|AAAAAAAALHFCAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    44.0|Friday|      N|                Y|      Y|AAAAAAAACCGEAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    17.0|Friday|      N|                Y|      Y|AAAAAAAAEAJBAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    25.0|Friday|      N|                Y|      Y|AAAAAAAAEIDCAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    26.0|Friday|      N|                Y|      Y|AAAAAAAAPDHDAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    72.0|Friday|      N|                Y|      Y|AAAAAAAAKDCEAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    48.0|Friday|      N|                Y|      Y|AAAAAAAACGNCAAAA|AAAAAAAAIAAAAAAA|              N|\n",
      "|1998-01-02|    58.0|Friday|      N|                Y|      Y|AAAAAAAACBEAAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    88.0|Friday|      N|                Y|      Y|AAAAAAAACMLBAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    54.0|Friday|      N|                Y|      Y|AAAAAAAAKPDDAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    69.0|Friday|      N|                Y|      Y|AAAAAAAAGKNCAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    68.0|Friday|      N|                Y|      Y|AAAAAAAAFCADAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    66.0|Friday|      N|                Y|      Y|AAAAAAAAABMBAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    59.0|Friday|      N|                Y|      Y|AAAAAAAAGEKAAAAA|AAAAAAAAIAAAAAAA|              N|\n",
      "|1998-01-02|    NULL|Friday|      N|                Y|      Y|AAAAAAAANJPAAAAA|AAAAAAAAHAAAAAAA|              N|\n",
      "|1998-01-02|    18.0|Friday|      N|                Y|      Y|AAAAAAAAECPBAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "|1998-01-02|    NULL|Friday|      N|                Y|      Y|AAAAAAAACGOBAAAA|AAAAAAAABAAAAAAA|              N|\n",
      "+----------+--------+------+-------+-----------------+-------+----------------+----------------+---------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "spc_typed_cleaned = spc_typed.select(\n",
    "    col(\"date\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"day\"),\n",
    "    col(\"holiday\"),\n",
    "    col(\"following_holiday\"),\n",
    "    col(\"weekend\"),\n",
    "    col(\"item_id\"),\n",
    "    col(\"store_id\"),\n",
    "    col(\"promo_indicator\")\n",
    ").sort(asc(\"date\"))\n",
    "\n",
    "spc_typed_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Preparation and Cleaning in Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>day</th>\n",
       "      <th>holiday</th>\n",
       "      <th>following_holiday</th>\n",
       "      <th>weekend</th>\n",
       "      <th>promo_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998-01-02</td>\n",
       "      <td>AAAAAAAAAAACAAAA</td>\n",
       "      <td>AAAAAAAACAAAAAAA</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998-01-02</td>\n",
       "      <td>AAAAAAAAAAHBAAAA</td>\n",
       "      <td>AAAAAAAAHAAAAAAA</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998-01-02</td>\n",
       "      <td>AAAAAAAAAAHDAAAA</td>\n",
       "      <td>AAAAAAAAHAAAAAAA</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998-01-02</td>\n",
       "      <td>AAAAAAAAAALDAAAA</td>\n",
       "      <td>AAAAAAAAHAAAAAAA</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-01-02</td>\n",
       "      <td>AAAAAAAAAAMCAAAA</td>\n",
       "      <td>AAAAAAAAIAAAAAAA</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           item_id          store_id  quantity     day holiday  \\\n",
       "0  1998-01-02  AAAAAAAAAAACAAAA  AAAAAAAACAAAAAAA      50.0  Friday       N   \n",
       "1  1998-01-02  AAAAAAAAAAHBAAAA  AAAAAAAAHAAAAAAA      90.0  Friday       N   \n",
       "2  1998-01-02  AAAAAAAAAAHDAAAA  AAAAAAAAHAAAAAAA      13.0  Friday       N   \n",
       "3  1998-01-02  AAAAAAAAAALDAAAA  AAAAAAAAHAAAAAAA      21.0  Friday       N   \n",
       "4  1998-01-02  AAAAAAAAAAMCAAAA  AAAAAAAAIAAAAAAA      15.0  Friday       N   \n",
       "\n",
       "  following_holiday weekend promo_indicator  \n",
       "0                 Y       Y               N  \n",
       "1                 Y       Y               N  \n",
       "2                 Y       Y               N  \n",
       "3                 Y       Y               N  \n",
       "4                 Y       Y               N  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spc_typed_cleaned.toPandas()\n",
    "df = df.dropna()\n",
    "df = df.groupby([\"date\", \"item_id\", \"store_id\"]).sum().reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2608127 entries, 0 to 2608126\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   date               object \n",
      " 1   item_id            object \n",
      " 2   store_id           object \n",
      " 3   quantity           float32\n",
      " 4   day                object \n",
      " 5   holiday            object \n",
      " 6   following_holiday  object \n",
      " 7   weekend            object \n",
      " 8   promo_indicator    object \n",
      "dtypes: float32(1), object(8)\n",
      "memory usage: 169.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Items: 9000\n",
      "Unique Stores: 6\n",
      "Unique Dates: 2608127\n"
     ]
    }
   ],
   "source": [
    "unique_items = df[\"item_id\"].nunique()\n",
    "print(f\"Unique Items: {unique_items}\")\n",
    "unique_stores = df[\"store_id\"].nunique()\n",
    "print(f\"Unique Stores: {unique_stores}\")\n",
    "unique_dates = df.index.nunique()\n",
    "print(f\"Unique Dates: {unique_dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "Cannot aggregate non-numeric type: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:371\u001b[0m, in \u001b[0;36mBaseWindow._prep_values\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mensure_float64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/algos_common_helper.pxi:42\u001b[0m, in \u001b[0;36mpandas._libs.algos.ensure_float64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Friday'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:489\u001b[0m, in \u001b[0;36mBaseWindow._apply_columnwise\u001b[0;34m(self, homogeneous_func, name, numeric_only)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:373\u001b[0m, in \u001b[0;36mBaseWindow._prep_values\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle this type -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Convert inf to nan for C funcs\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot handle this type -> object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m target_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m target_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:2127\u001b[0m, in \u001b[0;36mRolling.sum\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   2059\u001b[0m     template_header,\n\u001b[1;32m   2060\u001b[0m     create_section_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2125\u001b[0m     engine_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2126\u001b[0m ):\n\u001b[0;32m-> 2127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:1559\u001b[0m, in \u001b[0;36mRollingAndExpandingMixin.sum\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_apply(sliding_sum, engine_kwargs)\n\u001b[1;32m   1558\u001b[0m window_func \u001b[38;5;241m=\u001b[39m window_aggregations\u001b[38;5;241m.\u001b[39mroll_sum\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:723\u001b[0m, in \u001b[0;36mBaseWindowGroupby._apply\u001b[0;34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    717\u001b[0m     func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m--> 723\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# Reconstruct the resulting MultiIndex\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# 1st set of levels = group by labels\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# 2nd set of levels = original DataFrame/Series index\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     grouped_object_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:619\u001b[0m, in \u001b[0;36mBaseWindow._apply\u001b[0;34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_columnwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "File \u001b[0;32m/workspaces/Databricks-API-Exploration/.venv/lib/python3.12/site-packages/pandas/core/window/rolling.py:491\u001b[0m, in \u001b[0;36mBaseWindow._apply_columnwise\u001b[0;34m(self, homogeneous_func, name, numeric_only)\u001b[0m\n\u001b[1;32m    489\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_values(arr)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot aggregate non-numeric type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    494\u001b[0m res \u001b[38;5;241m=\u001b[39m homogeneous_func(arr)\n\u001b[1;32m    495\u001b[0m res_values\u001b[38;5;241m.\u001b[39mappend(res)\n",
      "\u001b[0;31mDataError\u001b[0m: Cannot aggregate non-numeric type: object"
     ]
    }
   ],
   "source": [
    "target_df = df.groupby([\"date\", \"item_id\", \"store_id\"]).rolling(window = 7).sum()\n",
    "\n",
    "target_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-container-template-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
